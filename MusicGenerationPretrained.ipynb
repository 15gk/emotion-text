{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/15gk/emotion-text/blob/main/MusicGenerationPretrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "d8q_PJKVLzT6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch transformers music21 pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import MusicgenForConditionalGeneration, AutoProcessor, Trainer, TrainingArguments\n",
        "from music21 import converter, instrument, note, chord\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "kAPVrWLJMJPX"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "rdyW71h9enY8"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EMOPIAMusicDataset(Dataset):\n",
        "    def __init__(self, midi_files, emotions, emotion_mapping):\n",
        "        self.midi_files = midi_files\n",
        "        self.emotions = emotions\n",
        "        self.emotion_mapping = emotion_mapping\n",
        "\n",
        "        # Create note-to-index mapping\n",
        "        self.notes = self._extract_all_notes()\n",
        "        self.note_to_index = {note: idx for idx, note in enumerate(self.notes)}\n",
        "        self.index_to_note = {idx: note for note, idx in self.note_to_index.items()}\n",
        "\n",
        "        # Extract and process note sequences\n",
        "        self.processed_sequences = self._process_midi_files()\n",
        "\n",
        "    def _extract_all_notes(self):\n",
        "        all_notes = []\n",
        "        for midi_file in self.midi_files:\n",
        "            for part in midi_file.parts:\n",
        "                for element in part.recurse():\n",
        "                    if isinstance(element, note.Note):\n",
        "                        all_notes.append(str(element.pitch))\n",
        "                    elif isinstance(element, chord.Chord):\n",
        "                        all_notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "\n",
        "        return sorted(list(set(all_notes)))\n",
        "\n",
        "    def _extract_notes(self, midi_file):\n",
        "        notes = []\n",
        "        for part in midi_file.parts:\n",
        "            for element in part.recurse():\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append(str(element.pitch))\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "        return notes\n",
        "\n",
        "    def _process_midi_files(self):\n",
        "        processed_sequences = []\n",
        "        for midi_file in self.midi_files:\n",
        "            notes = self._extract_notes(midi_file)\n",
        "            note_indices = [self.note_to_index[note] for note in notes]\n",
        "            processed_sequences.append(note_indices)\n",
        "        return processed_sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.midi_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.processed_sequences[idx]),\n",
        "            'labels': torch.tensor(self.emotions[idx] - 1)  # Convert to 0-based index\n",
        "        }\n"
      ],
      "metadata": {
        "id": "4QoZ34jhM7et"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'"
      ],
      "metadata": {
        "id": "8OolCQJtnu8l"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionMusicGenerationModel(nn.Module):\n",
        "    def __init__(self, num_notes, num_emotions, model_name=\"facebook/musicgen-small\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Emotion text mapping for generation\n",
        "        self.emotion_text_mapping = {\n",
        "            0: \"High energy, exciting, and happy music\",\n",
        "            1: \"Calm and positive music with gentle progression\",\n",
        "            2: \"Tense and dramatic music with intense feelings\",\n",
        "            3: \"Peaceful and soft music with subtle emotional depth\"\n",
        "        }\n",
        "\n",
        "        # Load pre-trained MusicGen model\n",
        "        self.musicgen_model = MusicgenForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "        # Custom embedding and classification layers\n",
        "        self.note_embedding = nn.Embedding(num_notes, 512)\n",
        "        self.emotion_classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),  # Add dropout for regularization\n",
        "            nn.Linear(256, num_emotions)\n",
        "        )\n",
        "\n",
        "        # Loss function\n",
        "        self.emotion_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids=None, labels=None):\n",
        "        if input_ids is not None:\n",
        "            # Embed note sequences\n",
        "            embedded_notes = self.note_embedding(input_ids)\n",
        "            pooled_notes = torch.mean(embedded_notes, dim=1)\n",
        "            emotion_logits = self.emotion_classifier(pooled_notes)\n",
        "\n",
        "            if labels is not None:\n",
        "                loss = self.emotion_loss(emotion_logits, labels)\n",
        "                _, predicted = torch.max(emotion_logits, 1)\n",
        "                accuracy = (predicted == labels).sum().item() / labels.size(0) * 100\n",
        "                return {'loss': loss, 'accuracy': accuracy}\n",
        "\n",
        "            return emotion_logits\n",
        "\n",
        "        # Generation logic\n",
        "        if labels is not None:\n",
        "            emotion_label = labels[0].item()\n",
        "            emotion_text = self.emotion_text_mapping[emotion_label]\n",
        "\n",
        "            inputs = self.processor(\n",
        "                text=[emotion_text],\n",
        "                audio=None,\n",
        "                sampling_rate=self.musicgen_model.config.audio_encoder.sampling_rate,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            audio_values = self.musicgen_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=250,\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            return audio_values"
      ],
      "metadata": {
        "id": "DGnE1GmiNVAF"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_emopia_dataset(label_path, midi_filepath):\n",
        "    \"\"\"\n",
        "    Prepare EMOPIA dataset for training\n",
        "\n",
        "    Args:\n",
        "        label_path (str): Path to labels CSV\n",
        "        midi_filepath (str): Path to MIDI files\n",
        "\n",
        "    Returns:\n",
        "        Prepared dataset and emotion mapping\n",
        "    \"\"\"\n",
        "    # Load labels\n",
        "    labels_df = pd.read_csv(label_path, delimiter=\",\")\n",
        "\n",
        "    # Emotion mapping\n",
        "    emotion_mapping = {\n",
        "        1: \"HVHA\",  # High Valence, High Arousal\n",
        "        2: \"HVLA\",  # High Valence, Low Arousal\n",
        "        3: \"LVHA\",  # Low Valence, High Arousal\n",
        "        4: \"LVLA\"   # Low Valence, Low Arousal\n",
        "    }\n",
        "\n",
        "    # Create file to emotion mapping\n",
        "    file_to_emotion = dict(zip(labels_df[\"ID\"], labels_df[\"4Q\"]))\n",
        "\n",
        "    # Load MIDI files\n",
        "    midi_files = []\n",
        "    file_emotions = []\n",
        "\n",
        "    for file in os.listdir(midi_filepath):\n",
        "        if file.endswith(\".mid\"):\n",
        "            file_id = file.split(\".mid\")[0]\n",
        "            if file_id in file_to_emotion:\n",
        "                full_path = os.path.join(midi_filepath, file)\n",
        "                midi = converter.parse(full_path)\n",
        "                midi_files.append(midi)\n",
        "                file_emotions.append(file_to_emotion[file_id])\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = EMOPIAMusicDataset(midi_files, file_emotions, emotion_mapping)\n",
        "\n",
        "    return dataset, emotion_mapping"
      ],
      "metadata": {
        "id": "uSEpr3rnNCDh"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(features):\n",
        "    # Extract input_ids and labels\n",
        "    input_ids = [item['input_ids'] for item in features]\n",
        "    labels = torch.tensor([item['labels'] for item in features])\n",
        "\n",
        "    # Pad input_ids to the maximum sequence length\n",
        "    padded_input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "        input_ids,\n",
        "        batch_first=True,\n",
        "        padding_value=0\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'input_ids': padded_input_ids.cpu(),  # Ensure CPU tensor\n",
        "        'labels': labels.cpu()  # Ensure CPU tensor\n",
        "    }"
      ],
      "metadata": {
        "id": "Qy6ZJi40pDwx"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataset, num_epochs=100, learning_rate=1e-4):\n",
        "    # Split dataset\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=4, collate_fn=custom_collate_fn)\n",
        "\n",
        "    # Move model to device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_train_accuracy = 0\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, labels)\n",
        "            loss = outputs['loss']\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            total_train_accuracy += outputs['accuracy']\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        total_val_accuracy = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(input_ids, labels)\n",
        "                total_val_loss += outputs['loss'].item()\n",
        "                total_val_accuracy += outputs['accuracy']\n",
        "\n",
        "        train_accuracy = total_train_accuracy / len(train_loader)\n",
        "        val_accuracy = total_val_accuracy / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {total_train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "        print(f\"Validation Loss: {total_val_loss/len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), 'emotion_music_generation_model.pth')\n",
        "    return model"
      ],
      "metadata": {
        "id": "aNSAnCAzNJEv"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/EMOPIA_1.0.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8eBWmv_kn_1",
        "outputId": "c3065249-0d51-4caf-bf14-0dcb0a767282"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/EMOPIA_1.0.zip\n",
            "replace __MACOSX/EMOPIA_1.0/._tagging_lists? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    label_path = \"/content/EMOPIA_1.0/label.csv\"\n",
        "    midi_filepath = \"/content/EMOPIA_1.0/midis\"\n",
        "\n",
        "    dataset, emotion_mapping = prepare_emopia_dataset(label_path, midi_filepath)\n",
        "\n",
        "    model = EmotionMusicGenerationModel(\n",
        "        num_notes=len(dataset.note_to_index),\n",
        "        num_emotions=len(emotion_mapping)\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    trained_model = train_model(model, dataset)"
      ],
      "metadata": {
        "id": "00WgFEMVkkH_"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfsNjXmik1yL",
        "outputId": "03455b7f-67bc-4a64-ed90-085a6c8c240a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/encodec/modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
            "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
            "  \"_name_or_path\": \"t5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Config of the audio_encoder: <class 'transformers.models.encodec.modeling_encodec.EncodecModel'> is overwritten by shared audio_encoder config: EncodecConfig {\n",
            "  \"_name_or_path\": \"facebook/encodec_32khz\",\n",
            "  \"architectures\": [\n",
            "    \"EncodecModel\"\n",
            "  ],\n",
            "  \"audio_channels\": 1,\n",
            "  \"chunk_length_s\": null,\n",
            "  \"codebook_dim\": 128,\n",
            "  \"codebook_size\": 2048,\n",
            "  \"compress\": 2,\n",
            "  \"dilation_growth_rate\": 2,\n",
            "  \"hidden_size\": 128,\n",
            "  \"kernel_size\": 7,\n",
            "  \"last_kernel_size\": 7,\n",
            "  \"model_type\": \"encodec\",\n",
            "  \"norm_type\": \"weight_norm\",\n",
            "  \"normalize\": false,\n",
            "  \"num_filters\": 64,\n",
            "  \"num_lstm_layers\": 2,\n",
            "  \"num_residual_layers\": 1,\n",
            "  \"overlap\": null,\n",
            "  \"pad_mode\": \"reflect\",\n",
            "  \"residual_kernel_size\": 3,\n",
            "  \"sampling_rate\": 32000,\n",
            "  \"target_bandwidths\": [\n",
            "    2.2\n",
            "  ],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"trim_right_ratio\": 1.0,\n",
            "  \"upsampling_ratios\": [\n",
            "    8,\n",
            "    5,\n",
            "    4,\n",
            "    4\n",
            "  ],\n",
            "  \"use_causal_conv\": false,\n",
            "  \"use_conv_shortcut\": false\n",
            "}\n",
            "\n",
            "Config of the decoder: <class 'transformers.models.musicgen.modeling_musicgen.MusicgenForCausalLM'> is overwritten by shared decoder config: MusicgenDecoderConfig {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"audio_channels\": 1,\n",
            "  \"bos_token_id\": 2048,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"dropout\": 0.1,\n",
            "  \"ffn_dim\": 4096,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_factor\": 0.02,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"musicgen_decoder\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_codebooks\": 4,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 2048,\n",
            "  \"scale_embedding\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 2048\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "Train Loss: 1.3596, Train Accuracy: 33.80%\n",
            "Validation Loss: 1.3731, Validation Accuracy: 31.94%\n",
            "Epoch 2/50\n",
            "Train Loss: 1.3233, Train Accuracy: 41.55%\n",
            "Validation Loss: 1.3236, Validation Accuracy: 39.35%\n",
            "Epoch 3/50\n",
            "Train Loss: 1.2865, Train Accuracy: 43.87%\n",
            "Validation Loss: 1.3102, Validation Accuracy: 36.11%\n",
            "Epoch 4/50\n",
            "Train Loss: 1.2560, Train Accuracy: 44.68%\n",
            "Validation Loss: 1.2706, Validation Accuracy: 43.06%\n",
            "Epoch 5/50\n",
            "Train Loss: 1.2179, Train Accuracy: 47.92%\n",
            "Validation Loss: 1.2348, Validation Accuracy: 43.98%\n",
            "Epoch 6/50\n",
            "Train Loss: 1.1914, Train Accuracy: 48.50%\n",
            "Validation Loss: 1.2176, Validation Accuracy: 44.91%\n",
            "Epoch 7/50\n",
            "Train Loss: 1.1308, Train Accuracy: 53.70%\n",
            "Validation Loss: 1.2325, Validation Accuracy: 43.98%\n",
            "Epoch 8/50\n",
            "Train Loss: 1.1307, Train Accuracy: 51.16%\n",
            "Validation Loss: 1.1843, Validation Accuracy: 46.30%\n",
            "Epoch 9/50\n",
            "Train Loss: 1.0901, Train Accuracy: 54.28%\n",
            "Validation Loss: 1.1630, Validation Accuracy: 47.69%\n",
            "Epoch 10/50\n",
            "Train Loss: 1.0974, Train Accuracy: 52.31%\n",
            "Validation Loss: 1.1790, Validation Accuracy: 47.69%\n",
            "Epoch 11/50\n",
            "Train Loss: 1.0689, Train Accuracy: 53.36%\n",
            "Validation Loss: 1.1544, Validation Accuracy: 47.69%\n",
            "Epoch 12/50\n",
            "Train Loss: 1.0563, Train Accuracy: 55.67%\n",
            "Validation Loss: 1.1437, Validation Accuracy: 48.61%\n",
            "Epoch 13/50\n",
            "Train Loss: 1.0363, Train Accuracy: 55.56%\n",
            "Validation Loss: 1.1706, Validation Accuracy: 48.61%\n",
            "Epoch 14/50\n",
            "Train Loss: 1.0236, Train Accuracy: 58.91%\n",
            "Validation Loss: 1.1478, Validation Accuracy: 48.15%\n",
            "Epoch 15/50\n",
            "Train Loss: 1.0256, Train Accuracy: 56.48%\n",
            "Validation Loss: 1.1655, Validation Accuracy: 47.69%\n",
            "Epoch 16/50\n",
            "Train Loss: 1.0026, Train Accuracy: 58.91%\n",
            "Validation Loss: 1.1392, Validation Accuracy: 48.15%\n",
            "Epoch 17/50\n",
            "Train Loss: 0.9929, Train Accuracy: 59.38%\n",
            "Validation Loss: 1.1239, Validation Accuracy: 50.46%\n",
            "Epoch 18/50\n",
            "Train Loss: 0.9873, Train Accuracy: 57.87%\n",
            "Validation Loss: 1.1293, Validation Accuracy: 50.46%\n",
            "Epoch 19/50\n",
            "Train Loss: 0.9858, Train Accuracy: 58.91%\n",
            "Validation Loss: 1.1235, Validation Accuracy: 51.39%\n",
            "Epoch 20/50\n",
            "Train Loss: 0.9697, Train Accuracy: 60.65%\n",
            "Validation Loss: 1.1238, Validation Accuracy: 52.78%\n",
            "Epoch 21/50\n",
            "Train Loss: 0.9513, Train Accuracy: 62.27%\n",
            "Validation Loss: 1.1128, Validation Accuracy: 50.93%\n",
            "Epoch 22/50\n",
            "Train Loss: 0.9567, Train Accuracy: 60.76%\n",
            "Validation Loss: 1.1182, Validation Accuracy: 49.07%\n",
            "Epoch 23/50\n",
            "Train Loss: 0.9342, Train Accuracy: 60.53%\n",
            "Validation Loss: 1.1193, Validation Accuracy: 50.93%\n",
            "Epoch 24/50\n",
            "Train Loss: 0.9369, Train Accuracy: 62.62%\n",
            "Validation Loss: 1.1135, Validation Accuracy: 52.31%\n",
            "Epoch 25/50\n",
            "Train Loss: 0.9294, Train Accuracy: 61.34%\n",
            "Validation Loss: 1.1489, Validation Accuracy: 49.54%\n",
            "Epoch 26/50\n",
            "Train Loss: 0.9213, Train Accuracy: 62.73%\n",
            "Validation Loss: 1.1114, Validation Accuracy: 50.93%\n",
            "Epoch 27/50\n",
            "Train Loss: 0.9124, Train Accuracy: 65.39%\n",
            "Validation Loss: 1.1152, Validation Accuracy: 50.00%\n",
            "Epoch 28/50\n",
            "Train Loss: 0.8878, Train Accuracy: 65.16%\n",
            "Validation Loss: 1.1117, Validation Accuracy: 51.85%\n",
            "Epoch 29/50\n",
            "Train Loss: 0.8951, Train Accuracy: 64.12%\n",
            "Validation Loss: 1.1261, Validation Accuracy: 50.00%\n",
            "Epoch 30/50\n",
            "Train Loss: 0.8826, Train Accuracy: 63.66%\n",
            "Validation Loss: 1.1129, Validation Accuracy: 50.00%\n",
            "Epoch 31/50\n",
            "Train Loss: 0.8628, Train Accuracy: 65.86%\n",
            "Validation Loss: 1.1308, Validation Accuracy: 50.93%\n",
            "Epoch 32/50\n",
            "Train Loss: 0.8584, Train Accuracy: 66.67%\n",
            "Validation Loss: 1.1132, Validation Accuracy: 51.85%\n",
            "Epoch 33/50\n",
            "Train Loss: 0.8631, Train Accuracy: 66.09%\n",
            "Validation Loss: 1.1044, Validation Accuracy: 53.24%\n",
            "Epoch 34/50\n",
            "Train Loss: 0.8535, Train Accuracy: 66.78%\n",
            "Validation Loss: 1.1145, Validation Accuracy: 50.00%\n",
            "Epoch 35/50\n",
            "Train Loss: 0.8352, Train Accuracy: 65.86%\n",
            "Validation Loss: 1.1085, Validation Accuracy: 50.46%\n",
            "Epoch 36/50\n",
            "Train Loss: 0.8307, Train Accuracy: 67.48%\n",
            "Validation Loss: 1.1145, Validation Accuracy: 50.00%\n",
            "Epoch 37/50\n",
            "Train Loss: 0.8248, Train Accuracy: 68.63%\n",
            "Validation Loss: 1.0969, Validation Accuracy: 52.78%\n",
            "Epoch 38/50\n",
            "Train Loss: 0.8228, Train Accuracy: 68.17%\n",
            "Validation Loss: 1.1268, Validation Accuracy: 50.46%\n",
            "Epoch 39/50\n",
            "Train Loss: 0.8196, Train Accuracy: 65.97%\n",
            "Validation Loss: 1.0981, Validation Accuracy: 50.46%\n",
            "Epoch 40/50\n",
            "Train Loss: 0.8031, Train Accuracy: 69.10%\n",
            "Validation Loss: 1.1108, Validation Accuracy: 50.46%\n",
            "Epoch 41/50\n",
            "Train Loss: 0.8066, Train Accuracy: 68.63%\n",
            "Validation Loss: 1.1190, Validation Accuracy: 50.46%\n",
            "Epoch 42/50\n",
            "Train Loss: 0.8009, Train Accuracy: 68.06%\n",
            "Validation Loss: 1.0974, Validation Accuracy: 51.85%\n",
            "Epoch 43/50\n",
            "Train Loss: 0.7909, Train Accuracy: 67.82%\n",
            "Validation Loss: 1.0826, Validation Accuracy: 54.63%\n",
            "Epoch 44/50\n",
            "Train Loss: 0.7886, Train Accuracy: 69.21%\n",
            "Validation Loss: 1.0872, Validation Accuracy: 54.17%\n",
            "Epoch 45/50\n",
            "Train Loss: 0.7973, Train Accuracy: 69.91%\n",
            "Validation Loss: 1.0889, Validation Accuracy: 53.24%\n",
            "Epoch 46/50\n",
            "Train Loss: 0.7716, Train Accuracy: 69.56%\n",
            "Validation Loss: 1.0805, Validation Accuracy: 55.09%\n",
            "Epoch 47/50\n",
            "Train Loss: 0.7715, Train Accuracy: 70.72%\n",
            "Validation Loss: 1.0970, Validation Accuracy: 51.85%\n",
            "Epoch 48/50\n",
            "Train Loss: 0.7734, Train Accuracy: 68.63%\n",
            "Validation Loss: 1.0795, Validation Accuracy: 53.24%\n",
            "Epoch 49/50\n",
            "Train Loss: 0.7582, Train Accuracy: 71.99%\n",
            "Validation Loss: 1.0872, Validation Accuracy: 54.17%\n",
            "Epoch 50/50\n",
            "Train Loss: 0.7601, Train Accuracy: 70.49%\n",
            "Validation Loss: 1.0861, Validation Accuracy: 55.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovV4UkUTgxSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}