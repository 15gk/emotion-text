{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\assma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\assma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('../data/emotion_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Rejoin words into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_safe(text):\n",
    "    try:\n",
    "        words = text.split()  # Split text into words\n",
    "        cleaned_words = [word for word in words if word.lower() not in stop_words]  # Remove stopwords\n",
    "        return ' '.join(cleaned_words)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return text  # Return original text in case of an error\n",
    "\n",
    "# Apply the function to the 'Text' column\n",
    "merged_df['Cleaned_Text'] = merged_df['Text'].apply(preprocess_text_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why ?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "      <td>Sage Act upgrade list tommorow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "      <td>WAY HOMEGIRL BABY FUNERAL!!! MAN HATE FUNERALS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "      <td>eye ! true hazel eye-and brilliant ! Regular f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz u .! babe n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I'm expecting an extremely important phonecall...</td>\n",
       "      <td>I'm expecting extremely important phonecall mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.Couldnt wait to see them live. If missing th...</td>\n",
       "      <td>.Couldnt wait see live. missing NH7 wasnt pain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>maken Tip 2: Stop op een moment dat je het hel...</td>\n",
       "      <td>maken Tip 2: Stop op een moment dat je het hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>En dan krijg je ff een cadeautje van een tweep...</td>\n",
       "      <td>En dan krijg je ff een cadeautje van een tweep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@1116am Drummer Boy bij op verzoek van @BiemO...</td>\n",
       "      <td>@1116am Drummer Boy bij op verzoek van @BiemOo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0                                             Why ?    \n",
       "1    Sage Act upgrade on my to do list for tommorow.   \n",
       "2  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...   \n",
       "3   Such an eye ! The true hazel eye-and so brill...   \n",
       "4  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b...   \n",
       "5  I'm expecting an extremely important phonecall...   \n",
       "6   .Couldnt wait to see them live. If missing th...   \n",
       "7  maken Tip 2: Stop op een moment dat je het hel...   \n",
       "8  En dan krijg je ff een cadeautje van een tweep...   \n",
       "9   @1116am Drummer Boy bij op verzoek van @BiemO...   \n",
       "\n",
       "                                        Cleaned_Text  \n",
       "0                                                  ?  \n",
       "1                    Sage Act upgrade list tommorow.  \n",
       "2  WAY HOMEGIRL BABY FUNERAL!!! MAN HATE FUNERALS...  \n",
       "3  eye ! true hazel eye-and brilliant ! Regular f...  \n",
       "4  @Iluvmiasantos ugh babe.. hugggzzz u .! babe n...  \n",
       "5  I'm expecting extremely important phonecall mi...  \n",
       "6  .Couldnt wait see live. missing NH7 wasnt pain...  \n",
       "7  maken Tip 2: Stop op een moment dat je het hel...  \n",
       "8  En dan krijg je ff een cadeautje van een tweep...  \n",
       "9  @1116am Drummer Boy bij op verzoek van @BiemOo...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[['Text', 'Cleaned_Text']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why ?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "      <td>Sage Act upgrade list tommorow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "      <td>WAY HOMEGIRL BABY FUNERAL!!! MAN HATE FUNERALS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "      <td>eye ! true hazel eye-and brilliant ! Regular f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz u .! babe n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0                                             Why ?    \n",
       "1    Sage Act upgrade on my to do list for tommorow.   \n",
       "2  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...   \n",
       "3   Such an eye ! The true hazel eye-and so brill...   \n",
       "4  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b...   \n",
       "\n",
       "                                        Cleaned_Text  \n",
       "0                                                  ?  \n",
       "1                    Sage Act upgrade list tommorow.  \n",
       "2  WAY HOMEGIRL BABY FUNERAL!!! MAN HATE FUNERALS...  \n",
       "3  eye ! true hazel eye-and brilliant ! Regular f...  \n",
       "4  @Iluvmiasantos ugh babe.. hugggzzz u .! babe n...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.to_csv('emotion_with_cleaned_text.csv', index=False)\n",
    "\n",
    "# Display the first few rows to verify the column is added and saved correctly\n",
    "merged_df[['Text', 'Cleaned_Text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\assma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('words')\n",
    "\n",
    "# Get a set of valid English words\n",
    "english_words = set(words.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to remove non-English words from the cleaned text\n",
    "def remove_non_english_words(text):\n",
    "    # Tokenize the text\n",
    "    words_in_text = text.split()\n",
    "\n",
    "    # Filter words that are in the English word set\n",
    "    filtered_words = [word for word in words_in_text if word in english_words]\n",
    "\n",
    "    # Rejoin the words into a single string\n",
    "    cleaned_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'Cleaned_Text' column\n",
    "merged_df['Cleaned_Text'] = merged_df['Cleaned_Text'].apply(remove_non_english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a new CSV file\n",
    "merged_df.to_csv('emotion_with_cleaned_text_no_non_english.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why ?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "      <td>upgrade list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "      <td>eye true hazel brilliant open countenance comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "      <td>ugh u babe ako e babe despite mas ko</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0                                             Why ?    \n",
       "1    Sage Act upgrade on my to do list for tommorow.   \n",
       "2  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...   \n",
       "3   Such an eye ! The true hazel eye-and so brill...   \n",
       "4  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b...   \n",
       "\n",
       "                                        Cleaned_Text  \n",
       "0                                                     \n",
       "1                                       upgrade list  \n",
       "2                                                     \n",
       "3  eye true hazel brilliant open countenance comp...  \n",
       "4               ugh u babe ako e babe despite mas ko  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows to verify\n",
    "merged_df[['Text', 'Cleaned_Text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_clean_df=pd.read_csv('../data/emotion_with_cleaned_text_no_non_english.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why ?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "      <td>upgrade list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "      <td>eye true hazel brilliant open countenance comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "      <td>ugh u babe ako e babe despite mas ko</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0                                             Why ?    \n",
       "1    Sage Act upgrade on my to do list for tommorow.   \n",
       "2  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...   \n",
       "3   Such an eye ! The true hazel eye-and so brill...   \n",
       "4  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b...   \n",
       "\n",
       "                                        Cleaned_Text  \n",
       "0                                                     \n",
       "1                                       upgrade list  \n",
       "2                                                     \n",
       "3  eye true hazel brilliant open countenance comp...  \n",
       "4               ugh u babe ako e babe despite mas ko  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Function to remove punctuation from text\n",
    "def remove_punctuation(text):\n",
    "    # Create a translation table to remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # Apply translation to remove punctuation\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Apply the function to the 'Cleaned_Text' column\n",
    "merged_df['Cleaned_Text'] = merged_df['Cleaned_Text'].apply(remove_punctuation)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "merged_df.to_csv('emotion_with_cleaned_text_no_punctuation.csv', index=False)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "merged_df[['Text', 'Cleaned_Text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_1=pd.read_csv('../data/emotion_with_cleaned_text_no_punctuation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why ?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "      <td>upgrade list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "      <td>eye true hazel brilliant open countenance comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "      <td>ugh u babe ako e babe despite mas ko</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0                                             Why ?    \n",
       "1    Sage Act upgrade on my to do list for tommorow.   \n",
       "2  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...   \n",
       "3   Such an eye ! The true hazel eye-and so brill...   \n",
       "4  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b...   \n",
       "\n",
       "                                        Cleaned_Text  \n",
       "0                                                NaN  \n",
       "1                                       upgrade list  \n",
       "2                                                NaN  \n",
       "3  eye true hazel brilliant open countenance comp...  \n",
       "4               ugh u babe ako e babe despite mas ko  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows where 'Cleaned_Text' is empty or contains only whitespace\n",
    "merged_df_2 = merged_df_1[merged_df_1['Cleaned_Text'].str.strip() != '']\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "merged_df_2.to_csv('emotion_with_cleaned_text_no_blank.csv', index=False)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "merged_df_2[['Text', 'Cleaned_Text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "      <td>upgrade list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "      <td>eye true hazel brilliant open countenance comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "      <td>ugh u babe ako e babe despite mas ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I'm expecting an extremely important phonecall...</td>\n",
       "      <td>extremely important minute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.Couldnt wait to see them live. If missing th...</td>\n",
       "      <td>wait see missing wasnt painful last gig</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "1    Sage Act upgrade on my to do list for tommorow.   \n",
       "3   Such an eye ! The true hazel eye-and so brill...   \n",
       "4  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b...   \n",
       "5  I'm expecting an extremely important phonecall...   \n",
       "6   .Couldnt wait to see them live. If missing th...   \n",
       "\n",
       "                                        Cleaned_Text  \n",
       "1                                       upgrade list  \n",
       "3  eye true hazel brilliant open countenance comp...  \n",
       "4               ugh u babe ako e babe despite mas ko  \n",
       "5                         extremely important minute  \n",
       "6            wait see missing wasnt painful last gig  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows where 'Cleaned_Text' is empty, contains only whitespace, or is NaN\n",
    "merged_df_2 = merged_df_1[merged_df_1['Cleaned_Text'].str.strip() != '']\n",
    "merged_df_2 = merged_df_2.dropna(subset=['Cleaned_Text'])\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "merged_df_2.to_csv('emotion_with_cleaned_text_no_blank_or_nan.csv', index=False)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "merged_df_2[['Text', 'Cleaned_Text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10=pd.read_csv('../data/emotion_with_cleaned_text_no_blank_or_nan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10['tokens']=df10['Cleaned_Text'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Cleaned_Text  \\\n",
      "0                                       upgrade list   \n",
      "1  eye true hazel brilliant open countenance comp...   \n",
      "2               ugh u babe ako e babe despite mas ko   \n",
      "3                         extremely important minute   \n",
      "4            wait see missing wasnt painful last gig   \n",
      "\n",
      "                                              tokens  \n",
      "0                                    [upgrade, list]  \n",
      "1  [eye, true, hazel, brilliant, open, countenanc...  \n",
      "2     [ugh, u, babe, ako, e, babe, despite, mas, ko]  \n",
      "3                     [extremely, important, minute]  \n",
      "4    [wait, see, missing, wasnt, painful, last, gig]  \n"
     ]
    }
   ],
   "source": [
    "print(df10[['Cleaned_Text','tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10.to_csv('emotion_with_tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11=pd.read_csv('../data/emotion_with_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for digits or non-word characters\n",
    "def check_non_word(text):\n",
    "    if bool(re.search(r'\\d', text)):  # Check for digits\n",
    "        return True\n",
    "    elif bool(re.search(r'[^\\w\\s]', text)):  # Check for non-word characters or punctuation\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11['contains_digits_or_non_word'] = df11['Cleaned_Text'].apply(check_non_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'Cleaned_Text' column and filter rows that match the condition\n",
    "rows_with_non_word = df10[df10['Cleaned_Text'].apply(check_non_word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Tweet_id, Emotion, Text, Cleaned_Text, tokens]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(rows_with_non_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lemmatization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\assma\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources if not already done\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to each token in the 'tokens' column\n",
    "df11['lemmatized_tokens'] = df11['Tokens'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Optionally, join the lemmatized tokens into a single string if needed\n",
    "df11['lemmatized_text'] = df11['lemmatized_tokens'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Cleaned_Text  \\\n",
      "0                                       upgrade list   \n",
      "1  eye true hazel brilliant open countenance comp...   \n",
      "2               ugh u babe ako e babe despite mas ko   \n",
      "3                         extremely important minute   \n",
      "4            wait see missing wasnt painful last gig   \n",
      "\n",
      "                                              Tokens  \\\n",
      "0                                ['upgrade', 'list']   \n",
      "1  ['eye', 'true', 'hazel', 'brilliant', 'open', ...   \n",
      "2  ['ugh', 'u', 'babe', 'ako', 'e', 'babe', 'desp...   \n",
      "3               ['extremely', 'important', 'minute']   \n",
      "4  ['wait', 'see', 'missing', 'wasnt', 'painful',...   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0              [ ' u p g r a d e ' ,   ' l i s t ' ]  \n",
      "1  [ ' e y e ' ,   ' t r u e ' ,   ' h a z e l ' ...  \n",
      "2  [ ' u g h ' ,   ' u ' ,   ' b a b e ' ,   ' a ...  \n",
      "3  [ ' e x t r e m e l y ' ,   ' i m p o r t a n ...  \n",
      "4  [ ' w a i t ' ,   ' s e e ' ,   ' m i s s i n ...  \n"
     ]
    }
   ],
   "source": [
    "# Display the updated DataFrame to check the results\n",
    "print(df11[['Cleaned_Text', 'Tokens', 'lemmatized_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to a new CSV file\n",
    "df11.to_csv('emotion_with_lemmatized_tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11['lemmatized_text1'] = df11['lemmatized_text'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Cleaned_Text  \\\n",
      "0                                       upgrade list   \n",
      "1  eye true hazel brilliant open countenance comp...   \n",
      "2               ugh u babe ako e babe despite mas ko   \n",
      "3                         extremely important minute   \n",
      "4            wait see missing wasnt painful last gig   \n",
      "\n",
      "                                              Tokens  \\\n",
      "0                                ['upgrade', 'list']   \n",
      "1  ['eye', 'true', 'hazel', 'brilliant', 'open', ...   \n",
      "2  ['ugh', 'u', 'babe', 'ako', 'e', 'babe', 'desp...   \n",
      "3               ['extremely', 'important', 'minute']   \n",
      "4  ['wait', 'see', 'missing', 'wasnt', 'painful',...   \n",
      "\n",
      "                                    lemmatized_text1  \n",
      "0                [ ' u p g r a d e ' , ' l i s t ' ]  \n",
      "1  [ ' e y e ' , ' t r u e ' , ' h a z e l ' , ' ...  \n",
      "2  [ ' u g h ' , ' u ' , ' b a b e ' , ' a k o ' ...  \n",
      "3  [ ' e x t r e m e l y ' , ' i m p o r t a n t ...  \n",
      "4  [ ' w a i t ' , ' s e e ' , ' m i s s i n g ' ...  \n"
     ]
    }
   ],
   "source": [
    "print(df11[['Cleaned_Text', 'Tokens', 'lemmatized_text1']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_extra_spaces(token):\n",
    "    # Remove spaces between characters in the token\n",
    "    cleaned_token = re.sub(r'\\s+', '', token)\n",
    "    return cleaned_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12=pd.read_csv('emotion_with_lemmatized_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12=df12.drop(['contains_digits_or_non_word', 'lemmatized_tokens', 'lemmatized_text'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12.to_csv('emotion_without_specific_columns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df13=pd.read_csv('../data/emotion_with_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neattext.functions as nfx\n",
    "\n",
    "# Remove the user handles\n",
    "df13['Clean_Text'] = df13['Text'].apply(nfx.remove_userhandles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTC_ADDRESS_REGEX',\n",
       " 'CURRENCY_REGEX',\n",
       " 'CURRENCY_SYMB_REGEX',\n",
       " 'Counter',\n",
       " 'DATE_REGEX',\n",
       " 'EMAIL_REGEX',\n",
       " 'EMOJI_REGEX',\n",
       " 'HASTAG_REGEX',\n",
       " 'MASTERCard_REGEX',\n",
       " 'MD5_SHA_REGEX',\n",
       " 'MOST_COMMON_PUNCT_REGEX',\n",
       " 'NUMBERS_REGEX',\n",
       " 'PHONE_REGEX',\n",
       " 'PoBOX_REGEX',\n",
       " 'SPECIAL_CHARACTERS_REGEX',\n",
       " 'STOPWORDS',\n",
       " 'STOPWORDS_de',\n",
       " 'STOPWORDS_en',\n",
       " 'STOPWORDS_es',\n",
       " 'STOPWORDS_fr',\n",
       " 'STOPWORDS_ru',\n",
       " 'STOPWORDS_yo',\n",
       " 'STREET_ADDRESS_REGEX',\n",
       " 'TextFrame',\n",
       " 'URL_PATTERN',\n",
       " 'USER_HANDLES_REGEX',\n",
       " 'VISACard_REGEX',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__generate_text',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__numbers_dict',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_lex_richness_herdan',\n",
       " '_lex_richness_maas_ttr',\n",
       " 'clean_text',\n",
       " 'defaultdict',\n",
       " 'digit2words',\n",
       " 'extract_btc_address',\n",
       " 'extract_currencies',\n",
       " 'extract_currency_symbols',\n",
       " 'extract_dates',\n",
       " 'extract_emails',\n",
       " 'extract_emojis',\n",
       " 'extract_hashtags',\n",
       " 'extract_html_tags',\n",
       " 'extract_mastercard_addr',\n",
       " 'extract_md5sha',\n",
       " 'extract_numbers',\n",
       " 'extract_pattern',\n",
       " 'extract_phone_numbers',\n",
       " 'extract_postoffice_box',\n",
       " 'extract_shortwords',\n",
       " 'extract_special_characters',\n",
       " 'extract_stopwords',\n",
       " 'extract_street_address',\n",
       " 'extract_terms_in_bracket',\n",
       " 'extract_urls',\n",
       " 'extract_userhandles',\n",
       " 'extract_visacard_addr',\n",
       " 'fix_contractions',\n",
       " 'generate_sentence',\n",
       " 'hamming_distance',\n",
       " 'inverse_df',\n",
       " 'lexical_richness',\n",
       " 'markov_chain',\n",
       " 'math',\n",
       " 'nlargest',\n",
       " 'normalize',\n",
       " 'num2words',\n",
       " 'random',\n",
       " 're',\n",
       " 'read_txt',\n",
       " 'remove_accents',\n",
       " 'remove_bad_quotes',\n",
       " 'remove_btc_address',\n",
       " 'remove_currencies',\n",
       " 'remove_currency_symbols',\n",
       " 'remove_custom_pattern',\n",
       " 'remove_custom_words',\n",
       " 'remove_dates',\n",
       " 'remove_emails',\n",
       " 'remove_emojis',\n",
       " 'remove_hashtags',\n",
       " 'remove_html_tags',\n",
       " 'remove_mastercard_addr',\n",
       " 'remove_md5sha',\n",
       " 'remove_multiple_spaces',\n",
       " 'remove_non_ascii',\n",
       " 'remove_numbers',\n",
       " 'remove_phone_numbers',\n",
       " 'remove_postoffice_box',\n",
       " 'remove_puncts',\n",
       " 'remove_punctuations',\n",
       " 'remove_shortwords',\n",
       " 'remove_special_characters',\n",
       " 'remove_stopwords',\n",
       " 'remove_street_address',\n",
       " 'remove_terms_in_bracket',\n",
       " 'remove_urls',\n",
       " 'remove_userhandles',\n",
       " 'remove_visacard_addr',\n",
       " 'replace_bad_quotes',\n",
       " 'replace_currencies',\n",
       " 'replace_currency_symbols',\n",
       " 'replace_dates',\n",
       " 'replace_emails',\n",
       " 'replace_emojis',\n",
       " 'replace_numbers',\n",
       " 'replace_phone_numbers',\n",
       " 'replace_special_characters',\n",
       " 'replace_term',\n",
       " 'replace_urls',\n",
       " 'string',\n",
       " 'term_freq',\n",
       " 'to_txt',\n",
       " 'unicodedata',\n",
       " 'word_freq',\n",
       " 'word_length_freq']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df13['Clean_Text'] = df13['Cleaned_Text'].apply(nfx.remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11.to_csv('emotion_with_tokens_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Cleaned_Text  \\\n",
      "0                                       upgrade list   \n",
      "1  eye true hazel brilliant open countenance comp...   \n",
      "2               ugh u babe ako e babe despite mas ko   \n",
      "3                         extremely important minute   \n",
      "4            wait see missing wasnt painful last gig   \n",
      "\n",
      "                                              Tokens  \\\n",
      "0                                ['upgrade', 'list']   \n",
      "1  ['eye', 'true', 'hazel', 'brilliant', 'open', ...   \n",
      "2  ['ugh', 'u', 'babe', 'ako', 'e', 'babe', 'desp...   \n",
      "3               ['extremely', 'important', 'minute']   \n",
      "4  ['wait', 'see', 'missing', 'wasnt', 'painful',...   \n",
      "\n",
      "                                          Clean_Text  \n",
      "0                                       upgrade list  \n",
      "1  eye true hazel brilliant open countenance comp...  \n",
      "2               ugh u babe ako e babe despite mas ko  \n",
      "3                         extremely important minute  \n",
      "4                     wait missing wasnt painful gig  \n"
     ]
    }
   ],
   "source": [
    "print(df13[['Cleaned_Text', 'Tokens', 'Clean_Text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = df13['Tokens']\n",
    "y = df13['Emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training the model<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3506334581053844"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression(max_iter=5000))])\n",
    "pipe_lr.fit(x_train,y_train)\n",
    "pipe_lr.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.338756118629427"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_svm = Pipeline(steps=[('cv',CountVectorizer()),('svc', SVC(kernel = 'rbf', C = 10))])\n",
    "pipe_svm.fit(x_train,y_train)\n",
    "pipe_svm.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3074431327382666"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rf = Pipeline(steps=[('cv',CountVectorizer()),('rf', RandomForestClassifier(n_estimators=10))])\n",
    "pipe_rf.fit(x_train,y_train)\n",
    "pipe_rf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "pipeline_file = open(\"text_emotion.pkl\",\"wb\")\n",
    "joblib.dump(pipe_lr,pipeline_file)\n",
    "pipeline_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
